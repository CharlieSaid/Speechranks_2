{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports Complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Imports Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (3392, 38)\n",
      "\n",
      "Missing values per column:\n",
      "Year                                 0\n",
      "Team1_Side                           0\n",
      "Team1_Member1_Rank                   0\n",
      "Team1_Member2_Rank                   0\n",
      "Team2_Side                           0\n",
      "Team2_Member1_Rank                   0\n",
      "Team2_Member2_Rank                   0\n",
      "Team1_Total_Wins                    98\n",
      "Team1_Total_Losses                  98\n",
      "Team1_Prelim_Wins                   98\n",
      "Team1_Prelim_Losses                 98\n",
      "Team1_Num_Tournaments               98\n",
      "Team1_Rank_Points                   98\n",
      "Team1_National_Rank                 98\n",
      "Team1_State_Rank                    98\n",
      "Team1_Win_Rate                      98\n",
      "Team1_Prelim_Win_Rate               98\n",
      "Team1_Total_Rounds                  98\n",
      "Team1_Avg_Points_Per_Tournament     98\n",
      "Team1_National_Exposure            188\n",
      "Team1_Avg_Tournament_Size          188\n",
      "Team1_Tournament_Points            188\n",
      "Team2_Total_Wins                   125\n",
      "Team2_Total_Losses                 125\n",
      "Team2_Prelim_Wins                  125\n",
      "Team2_Prelim_Losses                125\n",
      "Team2_Num_Tournaments              125\n",
      "Team2_Rank_Points                  125\n",
      "Team2_National_Rank                125\n",
      "Team2_State_Rank                   125\n",
      "Team2_Win_Rate                     125\n",
      "Team2_Prelim_Win_Rate              125\n",
      "Team2_Total_Rounds                 125\n",
      "Team2_Avg_Points_Per_Tournament    125\n",
      "Team2_National_Exposure            227\n",
      "Team2_Avg_Tournament_Size          227\n",
      "Team2_Tournament_Points            227\n",
      "target                               0\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "Year                                 int64\n",
      "Team1_Side                           int64\n",
      "Team1_Member1_Rank                   int64\n",
      "Team1_Member2_Rank                   int64\n",
      "Team2_Side                           int64\n",
      "Team2_Member1_Rank                   int64\n",
      "Team2_Member2_Rank                   int64\n",
      "Team1_Total_Wins                   float64\n",
      "Team1_Total_Losses                 float64\n",
      "Team1_Prelim_Wins                  float64\n",
      "Team1_Prelim_Losses                float64\n",
      "Team1_Num_Tournaments              float64\n",
      "Team1_Rank_Points                  float64\n",
      "Team1_National_Rank                float64\n",
      "Team1_State_Rank                   float64\n",
      "Team1_Win_Rate                     float64\n",
      "Team1_Prelim_Win_Rate              float64\n",
      "Team1_Total_Rounds                 float64\n",
      "Team1_Avg_Points_Per_Tournament    float64\n",
      "Team1_National_Exposure            float64\n",
      "Team1_Avg_Tournament_Size          float64\n",
      "Team1_Tournament_Points            float64\n",
      "Team2_Total_Wins                   float64\n",
      "Team2_Total_Losses                 float64\n",
      "Team2_Prelim_Wins                  float64\n",
      "Team2_Prelim_Losses                float64\n",
      "Team2_Num_Tournaments              float64\n",
      "Team2_Rank_Points                  float64\n",
      "Team2_National_Rank                float64\n",
      "Team2_State_Rank                   float64\n",
      "Team2_Win_Rate                     float64\n",
      "Team2_Prelim_Win_Rate              float64\n",
      "Team2_Total_Rounds                 float64\n",
      "Team2_Avg_Points_Per_Tournament    float64\n",
      "Team2_National_Exposure            float64\n",
      "Team2_Avg_Tournament_Size          float64\n",
      "Team2_Tournament_Points            float64\n",
      "target                               int64\n",
      "dtype: object\n",
      "\n",
      "Infinite values check:\n",
      "Year                               0\n",
      "Team1_Side                         0\n",
      "Team1_Member1_Rank                 0\n",
      "Team1_Member2_Rank                 0\n",
      "Team2_Side                         0\n",
      "Team2_Member1_Rank                 0\n",
      "Team2_Member2_Rank                 0\n",
      "Team1_Total_Wins                   0\n",
      "Team1_Total_Losses                 0\n",
      "Team1_Prelim_Wins                  0\n",
      "Team1_Prelim_Losses                0\n",
      "Team1_Num_Tournaments              0\n",
      "Team1_Rank_Points                  0\n",
      "Team1_National_Rank                0\n",
      "Team1_State_Rank                   0\n",
      "Team1_Win_Rate                     0\n",
      "Team1_Prelim_Win_Rate              0\n",
      "Team1_Total_Rounds                 0\n",
      "Team1_Avg_Points_Per_Tournament    0\n",
      "Team1_National_Exposure            0\n",
      "Team1_Avg_Tournament_Size          0\n",
      "Team1_Tournament_Points            0\n",
      "Team2_Total_Wins                   0\n",
      "Team2_Total_Losses                 0\n",
      "Team2_Prelim_Wins                  0\n",
      "Team2_Prelim_Losses                0\n",
      "Team2_Num_Tournaments              0\n",
      "Team2_Rank_Points                  0\n",
      "Team2_National_Rank                0\n",
      "Team2_State_Rank                   0\n",
      "Team2_Win_Rate                     0\n",
      "Team2_Prelim_Win_Rate              0\n",
      "Team2_Total_Rounds                 0\n",
      "Team2_Avg_Points_Per_Tournament    0\n",
      "Team2_National_Exposure            0\n",
      "Team2_Avg_Tournament_Size          0\n",
      "Team2_Tournament_Points            0\n",
      "target                             0\n",
      "dtype: int64\n",
      "Before dropping: 3392 rows\n",
      "After dropping missing values: 3005 rows\n",
      "Rows dropped: 387\n",
      "\n",
      "After cleaning - Missing values:\n",
      "0\n",
      "After cleaning - Infinite values:\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nr/k2xcn76959l8v4z4shd8k5sh0000gn/T/ipykernel_29422/3999815525.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df1['Team1_Side'] = df1['Team1_Side'].replace({'Aff': 1, 'Neg': 0})\n",
      "/var/folders/nr/k2xcn76959l8v4z4shd8k5sh0000gn/T/ipykernel_29422/3999815525.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df1['Team2_Side'] = df1['Team2_Side'].replace({'Aff': 1, 'Neg': 0})\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"rounds_joined.csv\")\n",
    "df1 = df.copy()\n",
    "\n",
    "# Make the last column the target (winner of the round)\n",
    "df1['target'] = df1['Team1_Won']\n",
    "\n",
    "# Drop the winner column\n",
    "df1 = df1.drop(columns=['Team1_Won', 'Team2_Won', 'Round_Number', 'Tournament_Name', 'Source_File', 'Team1_Code', 'Team2_Code', 'Team1_Member1_Name', 'Team1_Member2_Name', 'Team2_Member1_Name', 'Team2_Member2_Name', 'Team1_Member1_Points', 'Team1_Member2_Points', 'Team2_Member1_Points', 'Team2_Member2_Points'])\n",
    "\n",
    "# For Team1_Side and Team2_Side, replace all instances of 'Aff' with 1 and 'Neg' with 0.\n",
    "df1['Team1_Side'] = df1['Team1_Side'].replace({'Aff': 1, 'Neg': 0})\n",
    "df1['Team2_Side'] = df1['Team2_Side'].replace({'Aff': 1, 'Neg': 0})\n",
    "\n",
    "# DATA INSPECTION AND CLEANING\n",
    "print(\"Dataset shape:\", df1.shape)\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df1.isnull().sum())\n",
    "print(\"\\nData types:\")\n",
    "print(df1.dtypes)\n",
    "print(\"\\nInfinite values check:\")\n",
    "print(np.isinf(df1.select_dtypes(include=[np.number])).sum())\n",
    "\n",
    "# Handle missing values - DROP ROWS with missing data\n",
    "print(f\"Before dropping: {len(df1)} rows\")\n",
    "df1 = df1.dropna()\n",
    "print(f\"After dropping missing values: {len(df1)} rows\")\n",
    "print(f\"Rows dropped: {len(df.copy()) - len(df1)}\")\n",
    "\n",
    "# Handle any remaining infinite values\n",
    "df1 = df1.replace([np.inf, -np.inf], np.nan)\n",
    "df1 = df1.dropna()  # Drop any rows with infinite values too\n",
    "\n",
    "# ALTERNATIVE: Fill missing values with median (commented out)\n",
    "# Uncomment the lines below and comment out the dropna() approach above to switch back\n",
    "# numeric_columns = df1.select_dtypes(include=[np.number]).columns\n",
    "# df1[numeric_columns] = df1[numeric_columns].fillna(df1[numeric_columns].median())\n",
    "# df1 = df1.replace([np.inf, -np.inf], np.nan)\n",
    "# df1 = df1.fillna(0)\n",
    "\n",
    "print(\"\\nAfter cleaning - Missing values:\")\n",
    "print(df1.isnull().sum().sum())\n",
    "print(\"After cleaning - Infinite values:\")\n",
    "print(np.isinf(df1.select_dtypes(include=[np.number])).sum().sum())\n",
    "\n",
    "\n",
    "# Determine null model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature statistics after scaling:\n",
      "Mean: -0.000000\n",
      "Std: 1.000000\n",
      "Min: -4.624010\n",
      "Max: 10.784328\n"
     ]
    }
   ],
   "source": [
    "# FEATURE NORMALIZATION - Very important for neural networks!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate features and target\n",
    "X = df1.iloc[:, :-1]\n",
    "y = df1.iloc[:, -1]\n",
    "\n",
    "# Normalize features to have mean=0 and std=1\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Feature statistics after scaling:\")\n",
    "print(f\"Mean: {X_scaled.mean():.6f}\")\n",
    "print(f\"Std: {X_scaled.std():.6f}\")\n",
    "print(f\"Min: {X_scaled.min():.6f}\")\n",
    "print(f\"Max: {X_scaled.max():.6f}\")\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx])\n",
    "        y = torch.tensor(self.y[idx])\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2404\n",
      "Validation samples: 601\n",
      "Total batches per epoch: 76\n"
     ]
    }
   ],
   "source": [
    "dataset = CSVDataset(X_scaled, y.values)\n",
    "\n",
    "# Optional: Split into train/validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Total batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):  # Define a neural network class that inherits from PyTorch's nn.Module\n",
    "    def __init__(self, input_size, num_classes):  # Constructor method that takes input size and number of output classes\n",
    "        super().__init__()  # Call the parent class (nn.Module) constructor to initialize the neural network\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)  # Create first fully connected layer: transforms input_size features to 64 hidden units\n",
    "        self.fc3 = nn.Linear(32, num_classes)  # Create second fully connected layer: transforms 64 hidden units to num_classes outputs\n",
    "\n",
    "    def forward(self, x):  # Define the forward pass method that specifies how data flows through the network\n",
    "        x = F.relu(self.fc1(x))  # Pass input through first layer, then apply ReLU activation function (removes negative values)\n",
    "        x = F.relu(self.fc2(x))  # Pass through second layer with ReLU activation\n",
    "        return self.fc3(x)  # Pass the result through final layer and return the final output (no activation, raw logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Input size: 37\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "input_size = X_scaled.shape[1]  # Use the scaled data's feature count\n",
    "print(f\"Input size: {input_size}\")\n",
    "num_classes = len(np.unique(y))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "model = SimpleNet(input_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Check for any NaN in model parameters at initialization\n",
    "for name, param in model.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(f\"WARNING: NaN found in {name} at initialization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 0.5586\n",
      "Epoch 2/20 - Loss: 0.4225\n",
      "Epoch 3/20 - Loss: 0.3976\n",
      "Epoch 4/20 - Loss: 0.3886\n",
      "Epoch 5/20 - Loss: 0.3845\n",
      "Epoch 6/20 - Loss: 0.3780\n",
      "Epoch 7/20 - Loss: 0.3729\n",
      "Epoch 8/20 - Loss: 0.3682\n",
      "Epoch 9/20 - Loss: 0.3654\n",
      "Epoch 10/20 - Loss: 0.3611\n",
      "Epoch 11/20 - Loss: 0.3639\n",
      "Epoch 12/20 - Loss: 0.3559\n",
      "Epoch 13/20 - Loss: 0.3611\n",
      "Epoch 14/20 - Loss: 0.3521\n",
      "Epoch 15/20 - Loss: 0.3429\n",
      "Epoch 16/20 - Loss: 0.3450\n",
      "Epoch 17/20 - Loss: 0.3414\n",
      "Epoch 18/20 - Loss: 0.3369\n",
      "Epoch 19/20 - Loss: 0.3309\n",
      "Epoch 20/20 - Loss: 0.3268\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Check for NaN in input data\n",
    "        if torch.isnan(x_batch).any() or torch.isnan(y_batch).any():\n",
    "            print(f\"WARNING: NaN found in batch {batch_idx}\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        \n",
    "        # Check for NaN in outputs\n",
    "        if torch.isnan(outputs).any():\n",
    "            print(f\"WARNING: NaN in model outputs at batch {batch_idx}\")\n",
    "            break\n",
    "            \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Check for NaN in loss\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"WARNING: NaN loss at batch {batch_idx}\")\n",
    "            print(f\"Outputs: {outputs}\")\n",
    "            print(f\"Targets: {y_batch}\")\n",
    "            break\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "    if batch_count > 0:\n",
    "        avg_loss = running_loss / batch_count\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - No valid batches processed!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 81.03%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in val_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(x_batch)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "print(f\"Validation Accuracy: {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
